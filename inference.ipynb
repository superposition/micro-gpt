{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8178ba-2ea0-4f4f-b9ca-c3435a83bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918a651-5c6a-4a39-8b3e-a28259e4fd64",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c0ba50-83de-4ad7-b262-944e6d547ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4011.456 K parameters \n",
      " ModelConfig(dim=192, device='cpu', tokenizer='bpe', vocab_len=8192, num_layers=6, second_resid_norm=False, mlp_hidden_mult=4, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=2, num_kv_heads=1, head_dim=96, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (token_embedder): Embedding(8195, 192)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x Layer(\n",
       "      (pre_attn_norm): Norm()\n",
       "      (attn): MQA(\n",
       "        (Wq): Linear(in_features=192, out_features=192, bias=False)\n",
       "        (Wk): Linear(in_features=192, out_features=96, bias=False)\n",
       "        (Wv): Linear(in_features=192, out_features=96, bias=False)\n",
       "        (Wo): Linear(in_features=192, out_features=192, bias=False)\n",
       "      )\n",
       "      (pre_mlp_norm): Norm()\n",
       "      (mlp): MLP(\n",
       "        (Wup): Linear(in_features=192, out_features=512, bias=False)\n",
       "        (Wgate): Linear(in_features=192, out_features=512, bias=False)\n",
       "        (Wdown): Linear(in_features=512, out_features=192, bias=False)\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): Norm()\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### pretrained model options:\n",
    "## templateGPT\n",
    "# a 1m parameter model trained for 2000 iters with many layers, thin MLP hidden dimensions & few attention heads: 'models/templateGPT/trained/templateGPT_1m_tall_and_skinny'\n",
    "# a 1m parameter model trained for 2000 iters with layers, MLP hidden dimensions, & attention heads bw the other two: 'models/templateGPT/trained/templateGPT_1m_5ft11_and_skinnyfat'\n",
    "# a 1m parameter model trained for 2000 iters with few layers, thick MLP hidden dimensions & many attention heads: 'models/templateGPT/trained/templateGPT_1m_short_and_thicc'\n",
    "# a 2m parameter model trained for 4000 iters with CosineNorm: 'models/templateGPT/trained/templateGPT_2m_CosineNorm'\n",
    "# a 2m parameter model trained for 4000 iters with LayerNorm: 'models/templateGPT/trained/templateGPT_2m_LayerNorm'\n",
    "# a 2m parameter model trained for 4000 iters with RMSNorm: 'models/templateGPT/trained/templateGPT_2m_RMSNorm'\n",
    "# a 3m parameter model trained for 5000 iters with a gated MLP: 'models/templateGPT/trained/templateGPT_3m_GatedMLP'\n",
    "# a 3m parameter model trained for 5000 iters with an old-fashioned MLP: 'models/templateGPT/trained/templateGPT_3m_GatedMLP'\n",
    "# a 4m parameter model trained for 6000 iters with GeGLU activation: 'models/templateGPT/trained/templateGPT_4m_GeGLU'\n",
    "# a 4m parameter model trained for 6000 iters with SwiGLU activation: 'models/templateGPT/trained/templateGPT_4m_SwiGLU'\n",
    "## fractal-head-attention\n",
    "# a 1m parameter model designed to be compared against the equivalent templateGPT: 'models/fractal-head-attention/trained/FHA_1m_short_and_thicc'\n",
    "name = 'models/templateGPT/trained/templateGPT_4m_SwiGLU'\n",
    "\n",
    "from tools import load_model\n",
    "model, tokenizer, cfg = load_model(name)\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c971fce-8b3e-4732-bd66-d5d2028025d6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a366b1fc-b620-45a0-8b42-71bdca18906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing this model's specific generate function\n",
    "from tools import import_from_nested_path\n",
    "path_parts = name.split('/')\n",
    "imported_objects = import_from_nested_path([path_parts[0], path_parts[1]], 'inference', ['generate'])\n",
    "generate = imported_objects.get('generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c202ea0-e64d-4367-a4a6-102756fe63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once| |upon| |a| |ti|me|, |th|er|e| |was| |a| |boy| |na|me|d| |Tim|. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time, there was a boy named Tim. \"\n",
    "print(tokenizer.display(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b13a76-50f8-48b6-b7cf-9097d307c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum attention matrix size in memory will be 64x512 rather than 512x512\n",
      "\n",
      "Once upon a time, there was a boy named Tim. He was a very special boy who loved to play with his toy car. One day, Tim found a big box of toys on the floor. He wanted to keep it safe.\n",
      "Tim decided to show the box to his friend, Sam. Sam said, \"I will bring it to the box.\" They worked together to put the toys away. They went to the box and started to make the toys go outside.\n",
      "Tim and Sam were very happy. They all played together and had lots of fun. They were not happy and happy. They learned that even when they need it to bring everyone more. Tim and Sam were proud of their new toys and had a great time. And they lived happily ever after.\n"
     ]
    }
   ],
   "source": [
    "output = generate(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer,\n",
    "    #max_gen_len = 100,\n",
    "    temperature = 0.7,\n",
    "    memory_saver_div = 8,\n",
    "    top_p = 0.9,\n",
    "    top_k = 32,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114de8bf-ab76-460d-8c37-ebe368b47e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
